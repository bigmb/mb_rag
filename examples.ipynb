{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malav/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mb_rag.chatbot.basic import conversation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mconversation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-4o'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'openai'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A class to represent a conversation model\n",
      "Attributes:\n",
      "    chatbot (ChatOpenAI): Chatbot model\n",
      "    context (str): Context of the conversation\n",
      "    question (str): Question to ask\n",
      "    message_list (list): List of messages in the conversation\n",
      "    file_path (str): Path to the conversation file (if s3_path then add s3_path='loc' and client and bucket)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/mb_packages/mb_rag/mb_rag/chatbot/basic.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "conversation_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation loaded from file: ./examples/test1.txt\n",
      "Would you like to solve another problem or is there something else you need help with?\n"
     ]
    }
   ],
   "source": [
    "load_chat = conversation_model(model_name=\"gpt-4o\", model_type=\"openai\",file_path=\"./examples/test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='addition\\n'),\n",
       " SystemMessage(content='\\n'),\n",
       " SystemMessage(content='what is 4+4?\\n'),\n",
       " SystemMessage(content='\\n'),\n",
       " SystemMessage(content='4 + 4 equals 8.\\n'),\n",
       " SystemMessage(content='\\n'),\n",
       " SystemMessage(content='Is there anything else you would like to know or calculate?\\n'),\n",
       " SystemMessage(content='what is 4+5?\\n'),\n",
       " SystemMessage(content='4 + 5 equals 9.\\n'),\n",
       " AIMessage(content='Would you like to solve another problem or is there something else you need help with?')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chat.get_all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 + 13 equals 25.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12 + 13 equals 25.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chat.add_message('what is 12+13?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='addition\\n'),\n",
       " SystemMessage(content='\\n'),\n",
       " SystemMessage(content='what is 4+4?\\n'),\n",
       " SystemMessage(content='\\n'),\n",
       " SystemMessage(content='4 + 4 equals 8.\\n'),\n",
       " SystemMessage(content='\\n'),\n",
       " SystemMessage(content='Is there anything else you would like to know or calculate?\\n'),\n",
       " SystemMessage(content='what is 4+5?\\n'),\n",
       " SystemMessage(content='4 + 5 equals 9.\\n'),\n",
       " AIMessage(content='Would you like to solve another problem or is there something else you need help with?'),\n",
       " HumanMessage(content='what is 12+13?'),\n",
       " AIMessage(content='12 + 13 equals 25.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chat.get_all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 path given : None\n",
      "Conversation saved to file as s3_path not given: ./examples/test1.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chat.save_conversation(file_path=\"./examples/test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in October 2023, CursorAI is not a widely recognized or established term or product in the public domain. It’s possible that CursorAI could refer to a specialized or emerging technology, software, or tool that involves artificial intelligence (AI) in relation to cursor movements, user interfaces, or some other application.\n",
      "\n",
      "If you have more specific context or details about CursorAI, I can provide more tailored information. It’s also worth checking the latest sources or official announcements if this is a very recent development.\n"
     ]
    }
   ],
   "source": [
    "new_chat = conversation_model(model_name=\"gpt-4o\", model_type=\"openai\",context=\"You are a helpful assistant\",question=\"what is cursorai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 path given : None\n",
      "Conversation saved to file as s3_path not given: ./examples/test2.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chat.save_conversation(file_path=\"./examples/test2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_rag.rag.embeddings import embedding_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0membedding_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'openai'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'text-embedding-3-small'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvector_store_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'chroma'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      Class to generate embeddings for the RAG model\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/mb_packages/mb_rag/mb_rag/rag/embeddings.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "embedding_generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malav/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "em_gen = embedding_generator(model=\"openai\",model_type=\"text-embedding-3-small\",vector_store_type=\"chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File already exists'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_gen.generate_text_embeddings(text_data_path=['./examples/sample_test.txt'],chunk_size=100,\n",
    "                                folder_save_path='./examples/sample_test_embeddings',replace_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_loading = em_gen.load_embeddings(embeddings_folder_path='./examples/sample_test_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x79b34b529590>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        VectorStoreRetriever\n",
      "\u001b[0;31mString form:\u001b[0m tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma o <...> 4c379a10> search_type='similarity_score_threshold' search_kwargs={'k': 3, 'score_threshold': 0.9}\n",
      "\u001b[0;31mFile:\u001b[0m        ~/.local/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\n",
      "\u001b[0;31mDocstring:\u001b[0m   Base Retriever class for VectorStore."
     ]
    }
   ],
   "source": [
    "em_gen.retriever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_retriever = em_gen.load_retriever(embeddings_folder_path='./examples/sample_test_embeddings',search_params={\"k\": 3,\"score_threshold\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malav/.local/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:559: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': './examples/sample_test.txt'}, page_content=\"For Further Exploration\\nTo learn more about LangChain and its wide range of applications, be sure to check out the comprehensive documentation and tutorials available on the official website:\\n\\nLangChain Documentation: https://python.langchain.com/\\nDon't Forget to Like and Subscribe!\\nIf you're looking for in-depth tutorials and insights into LangChain, CrewAI, and other AI technologies, be sure to check out the fantastic YouTube channel by Brandon Hancock:\\n\\nYouTube Channel: https://www.youtube.com/@bhancock_ai\\nDon't forget to like and subscribe to his channel!!\"), -0.1544140213897769), (Document(metadata={'source': './examples/sample_test.txt'}, page_content='Chatbots and Conversational Agents: Build intelligent chatbots capable of understanding natural language and providing informative responses.\\nQuestion Answering Systems: Develop systems that can accurately answer questions posed in natural language, leveraging information from various sources.\\nSummarization Tools: Create tools that can condense lengthy documents or articles into concise summaries.\\nText Generation Applications: Build applications that can generate creative content like stories, poems, or code snippets.\\nAutonomous Agents: Develop agents that can perform tasks autonomously based on user instructions or predefined goals.\\nA Wealth of Resources\\nLangChain boasts a thriving community and comprehensive documentation, making it easy for developers to get started and explore its capabilities. It offers extensive tutorials, examples, and guides to help users build powerful LLM-powered applications.'), -0.1612710696822559), (Document(metadata={'source': './examples/sample_test.txt'}, page_content='LangChain: A Framework for LLM-Powered Applications\\nLangChain is a powerful and flexible framework designed to simplify the development of applications that harness the capabilities of large language models (LLMs). It provides a wide range of tools, abstractions, and integrations that help developers build, customize, and optimize applications that leverage LLMs for tasks like text generation, question answering, summarization, chatbots, and more.'), -0.18063532841070784)]\n",
      "  warnings.warn(\n",
      "/home/malav/.local/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:571: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_retriever.invoke(input=\"what is the article about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
